\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{listings, lstautogobble}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}

\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\parindent0pt
\parskip2pt
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
\lstset{language=Java,
    basicstyle=\scriptsize\ttfamily,
    commentstyle=\ttfamily\itshape\color{gray},
    stringstyle=\ttfamily,
    showstringspaces=false,
    breaklines=true,
    frameround=ffff,
    frame=single,
    rulecolor=\color{black},
    autogobble=true
}


%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\begin{document}

\begin{center}{\huge{\textbf{CS5234 Cheat Sheet \small{Gabriel Yeo}}}}\\
\end{center}
\begin{multicols*}{3}

    \tikzstyle{mybox} = [draw=black, fill=white, very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=10pt]
    \tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    \begin{tabular}{lp{5cm} l}
                        \textit{Markov:}
                         & $P(X \geq \alpha) \leq \frac{E(X)}{\alpha}$                                                      \\
                        \textit{Chebyshev:}
                         & $P(|X - \mu| \geq k) \leq \frac{Var(X)}{k^2}$                                                    \\
                        \textit{Chernoff:}
                         & $P(X \leq (1-\delta)\mu) \leq e^{-\frac{\delta^2\mu}{2}} \leq e^{-\frac{\delta^2\mu}{3}}$        \\
                         & $P(X \geq (1+\delta)\mu) \leq e^{-\frac{\delta^2\mu}{2+\delta}} \leq e^{-\frac{\delta^2\mu}{3}}$ \\
                         & $X = X_1 + ... + X_n$,                                                                           \\
                         & $X_i \in \{0, 1\}$, (ind. Bernoulli)                                                             \\
                        \textit{Chernoff:}
                         & $P(|X - \mu| \geq \delta\mu) \leq 2e^{-\frac{\delta^2\mu}{3}}$                                   \\
                        \textit{Hoeffding:}
                         & $P(|X - \mu| \geq t) \leq 2e^{\frac{-2t^2}{n}}$                                                  \\
                         & $X = X_1 + ... + X_n$,                                                                           \\
                         & $X_i \in [0, 1]$                                                                                 \\
                        \textit{Hoeffding:}
                         & $P(|\overline X - E[\overline X]| \geq t) \leq 2e^{-2nt^2}$                                      \\
                         & $\overline X = \frac{1}{n}(X_1 + ... + X_n)$                                                     \\
                        \textit{Hoeffding (gen):}
                         & $P(|\overline X - E[\overline X]| \geq t) \leq 2e^{-\frac{2t^2}{\sum (a_i - b_i)^2}}$            \\
                         & $X_i \in [a_i, b_i]$                                                                             \\
                    \end{tabular}
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Inequalities};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    \begin{tabular}{lp{4cm} l}
                        \textit{Taylor expansion}:
                         & $e^{x} = 1 + x + \frac{x^2}{2!} + ... = \sum\limits^{\infty} \frac{x^n}{n!}$ \\
                        \textit{For $0 < x \leq 1$}:
                         & $1-x \leq e^{-x} \leq 1 - \frac{x}{2}$                                       \\
                        \textit{For $0 < x \leq 1$}:
                         & $\frac{1}{e^{2}} \leq (1-x)^{1/x} \leq \frac{1}{e}$                          \\
                        \textit{Approx $\binom{a}{b}$}:
                         & $(\frac{a}{b})^b \leq \binom{a}{b} \leq (\frac{ea}{b})^b$                    \\
                        \textit{For $0 < x < 1$}:
                         & $\sum\limits_{i=0}^{\infty} a^i = \frac{1}{1-a}$                             \\
                        \textit{Natural log}:
                         & $ln(n-1) \leq \sum\limits_{i=1}^{n} \frac{1}{i} \leq ln(n) + 1$              \\
                        \textit{Powers of 2}:
                         & $\sum\limits_{i=0}^{n}2^i = 2^{n+1}-1$                                       \\
                        \textit{$1/e$}:
                         & $= 0.36787944117$                                                            \\
                        \textit{$1/e^2$}:
                         & $= 0.13533528323 \leq 1/6$                                                   \\
                        \textit{$1/e^3$}:
                         & $= 0.04978706836 \leq 1/20$                                                  \\
                        \textit{Variance}:
                         & $Var[X] = E[(X - E[X])^2)]$                                                  \\
                         & $= E[X^2] - E[X]$
                    \end{tabular}
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Facts};
    \end{tikzpicture}

    % \begin{tikzpicture}
    %     \node [mybox] (box){%
    %         \begin{minipage}{0.3\textwidth}
    %             \small{
    %                 \begin{tabular}{lp{4cm} l}
    %                     \textit{Expectation}:
    %                      & $E[X] = \sum\limits_{v \in D} v \cdot Pr[X=v]$ \\

    %                     \textit{Variance}:
    %                      & $Var[X] = E[(X - E[X])^2)]$                    \\
    %                      & $= E[X^2] - E[X]$                              \\

    %                     \textit{More variance}:
    %                      & $Var[aX] = a^2Var[X]$                          \\

    %                     \textit{Linearity of expectation}:
    %                      & $E[\sum X_i] = \sum E[X_i] $                   \\

    %                     \textit{If independent $X_i$}:
    %                      & $Var[\sum X_i] = \sum Var[X_i] $               \\

    %                     \textit{Conditional}:
    %                      & $P[X and Y] = P[X|Y]\cdot P[Y]$                \\

    %                     \textit{Independence}:
    %                      & $P[X and Y] = P[X]\cdot P[Y]$                  \\

    %                     \textit{Union bound}:
    %                      & $P[\bigcup E_i] \leq \sum P[E_i]$
    %                 \end{tabular}
    %             }
    %         \end{minipage}
    %     };
    %     \node[fancytitle, right=10pt] at (box.north west) {Probability};
    % \end{tikzpicture}

    % \vfill\null
    % \columnbreak

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    \begin{tabular}{|l|c|c|}\hline
                        \textbf{Algorithm}  & \textbf{Classical}     & \textbf{Approx}                  \\ \hline
                        BFS                 & $O(n+m)$               &                                  \\
                        Connected?          & $O(n+m)$               & $O(\frac{1}{\epsilon^2d})$       \\
                        \# CCs              & $O(n+m)$               & $O(\frac{d}{\epsilon^3})$        \\
                        MST weight          & $O(mlogn)$             & $O(\frac{dW^4logW}{\epsilon^3})$ \\
                        Prim's              & $O((m + n)\cdot logn)$ &                                  \\
                        Prim's (Fib heap)   & $O(m + nlogn)$         &                                  \\
                        Kruskal's (MST)     & $O(mlogn)$             &                                  \\
                        Dijkstra            & $O((m + n)\cdot logn)$ &                                  \\
                        Dijkstra (Fib heap) & $O(m + nlogn)$         &                                  \\
                        \hline
                    \end{tabular}
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Complexities};
    \end{tikzpicture}

    % \begin{tikzpicture}
    %     \node [mybox] (box){%
    %         \begin{minipage}{0.3\textwidth}
    %             \small{
    %                 Give an array $A$ with $n$ elements, $A[i] \in {0, 1}$: \\
    %                 (1) If array has $\geq \epsilon n$ 1's, return False with probability at least $1-\delta$: \\
    %                 Assume $\geq \epsilon n$ 1's, then for sample $i$: \\
    %                 $Pr[A[i] = 1] \geq \epsilon n/n \geq \epsilon$.
    %                 \begin{align*}
    %                     Pr(\text{all samples are 0})
    %                      & \leq (1-\epsilon)^s                               \\
    %                      & \leq (1-\epsilon)^{\frac{ln(1/\delta)}{\epsilon}} \\
    %                      & \leq e^{-ln(1/\delta)}                            \\
    %                      & \leq \delta
    %                 \end{align*}
    %                 Fix $s = \frac{ln(1/\delta)}{\epsilon}$
    %             }
    %         \end{minipage}
    %     };
    %     \node[fancytitle, right=10pt] at (box.north west) {Lecture 1 All 0s};
    % \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Give an array $A$ with $n$ elements, $A[i] \in {0, 1}$: \\
                    (1) Find number of 1's $\pm \epsilon$ with probability at least $1-\delta$. \\
                    Let $Y_i$ be $s$ independent samples in $[0, 1]$. \\
                    Output = $Z = 1/s \sum Y_i$ \\
                    Probability of failure:
                    \begin{align*}
                        Pr(|Z - E[Z]| \geq \epsilon) & \leq 2e^{-2s\epsilon^2}                                \\
                                                     & \leq 2e^{-2\frac{ln(2/\delta)}{2\epsilon^2}\epsilon^2} \\
                                                     & \leq 2e^{-ln(2/\delta)}                                \\
                                                     & \leq \delta
                    \end{align*}
                    Fix $s = \frac{ln(2/\delta)}{2\epsilon^2}$
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 1 Number of 1s};
    \end{tikzpicture}

    % \vfill\null
    % \columnbreak

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Output CC such that: $|CC(G) - C| \leq \epsilon n$, w.p. > $1 - \delta$ \\
                    Define $n(u)$ = num nodes in the CC of $u$. \\
                    $cost(u) = 1/n(u)$. \\
                    $\sum\limits_{u \in CC_i} cost(u) = 1$.\\
                    \begin{lstlisting}[mathescape]
            sum = 0
            for $j$ = 1 to $s$:
                - Sample $u$ randomly 
                - BFS from $u$, stop when see up to $2/\epsilon$ nodes
                - If BFS found $> 2/\epsilon$ node:
                    - sum := sum + $\epsilon/2$
                - Else:
                    - sum := sum + cost($u$)
            return n $\cdot$ (sum/s)
        \end{lstlisting}
                    Let $\overline C = \sum cost(u_j)$ \\
                    Let $Y_j = cost(u_j)$ of our sample $j$ \\
                    $|CC(G) - \overline C| \leq \epsilon n /2$ \\
                    $E[Y_j] = \sum \frac{1}{n} cost(u_i) = \frac{1}{n} \overline C$ \\
                    $E[\sum Y_j] = s \cdot E[Y_j] = \frac{s}{n} \overline C$ \\
                    Since we output $\frac{n}{s} \sum Y_j$, we get $E[\frac{n}{s} \sum Y_j] = \overline C$
                    \begin{align*}
                         & P(|\overline C - \frac{n}{s} \sum Y_j| > \epsilon n/2)  \\
                         & = P(|E[\sum Y_j] - \sum Y_j| > \frac{s}{n}\epsilon n/2) \\
                         & = P(|E[\sum Y_j] - \sum Y_j| > s\epsilon/2)             \\
                         & \leq 2e^{-2\epsilon^2s^2/4s}                            \\
                         & \leq 2e^{-\epsilon^2s/2} \leq \delta
                    \end{align*}
                    Set $s = \frac{2}{\epsilon^2}ln(2/\delta)$. \\
                    Complexity = $2d/\epsilon \cdot O(\frac{1}{\epsilon^2}ln(1/\delta)) = O(\frac{d}{\epsilon^3}ln(1/\delta))$.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 2 Connected Components};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    $G$ is \epsilon-close to connected if you can modify at most $\epsilon nd$ entries in the adjaceny list to make it connected.
                    If $G(V, E)$ is connected, output True, else if \epsilon-far from connected, output False.
                    \begin{lstlisting}[mathescape]
        Connected(G, n, d, $\epsilon$)
            Repeat $16/\epsilon d$ times:
                - Choose a random node $u$
                - Do a BFS from $u$, stopping after $8/\epsilon d$ nodes seen.
                - If CC of $u$ has $\leq 8/\epsilon d$ nodes, return FALSE.
            return TRUE
        \end{lstlisting}
                    BFS cost = $\frac{8}{\epsilon d} \cdot d$ \\
                    Total complexity = $O(\frac{1}{\epsilon^2d})$
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 2 Connectivity};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Output $M$ such that: $M = MST(G)(1 \pm \epsilon)$ w.p. > $1 - \delta$. \\
                    Let $G_j$ be the graph with edge weights $j$ and below. \\
                    Let $C_j$ be the connected components in $G_j$. \\
                    $MST(G)$ contains $C_j - 1$ edges of weight > $j$. \\
                    Therefore:
                    \begin{align*}
                        MST(G) & = (n - C_1) + \sum\limits_{j-1}^{W-1}(j+1)(C_j-C_j+1) \\
                               & = n - W + \sum\limits_{j-1}^{W-1}C_j
                    \end{align*}
                    \begin{lstlisting}[mathescape]
            sum := $n$ - $W$
            for $j$ = 1 to $W$ - 1:
                - $X_j$ = ApproxCC($G_j, d, \epsilon' \delta'$)
                - sum := sum + $X_j$
            return sum
            \end{lstlisting}
                    Sum of errors: $(\epsilon'n)(W-1)$ \\
                    Set $\epsilon' = \epsilon/W$, then sum of errors $\leq \epsilon n$. \\
                    Set $\delta' = \delta/W$. Then $P(any fail) \leq \sum\limits_{1}^{W-1} \delta/W \leq \delta$. \\
                    $MST(G) - \epsilon n \leq sum \leq MST(G) + \epsilon n$ \\
                    Since $MST(G) \geq n - 1 \geq n/2$, $n \leq 2MST(G)$, \\
                    $MST(G)(1-2\epsilon) \leq sum \leq MST(G)(1+2\epsilon)$
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 3 MST weight};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    If graph $H$ has $girth(H) > 2k \rightarrow H$ has $O(n^{1+1/k})$ edges. \\
                    log($n$)-spanner space with $k$ = log($n$): requires $O(nlogn)$ space.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 5 Spanner};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Return the size of the Maximal Matching.
                    \begin{lstlisting}[mathescape]
            query($e$):
                for all neighbours $e'$ of $e$:
                    if hash($e'$) < hash($e$)
                        if query($e'$) = TRUE
                            return FALSE
                return TRUE

            sum := 0
            for $j$ = 1 to $s$:
                - Choose edge $e$ uniformly at random.
                - if (query($e$) = True)
                    sum := sum + 1
            return m$\cdot$sum/s
            \end{lstlisting}
                    $E[cost] = 2\sum\limits_{k=1}^{\infty} \frac{d^k}{k!} = O(e^d)$. \\
                    $sum = MM(G) \pm \epsilon m$. \\
                    Complexity = $O(\frac{e^d}{\epsilon^2})$. \\
                    Can do better: $O(\frac{d^4}{\epsilon^2})$, even $O(\frac{d^2}{\epsilon^2})$, and reduce error to $\pm \epsilon n$.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 3 Maximal Matching};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Every randomized algorithm on a worst-case input is always slower than
                    the best deterministic algorithm on the worst distribution.
                    \begin{multline*}
                        \forall A \in R : \max\limits_{x \in X}(E[cost(A,x)]) \\
                        \geq \min\limits_{B \in D}(E[cost(B, x chosen from \gamma)])
                    \end{multline*}
                    Recipe:
                    \begin{itemize}
                        \item Choose distribution $\gamma$.
                        \item Show that the expected cost of every deterministic algorithm from $\gamma$
                              is greater than some cost $c$.
                        \item Conclude that every randomized algorithm has at least one input with expected cost
                              at least as bad as $c$.
                    \end{itemize}
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 3 Yao's Mini-Max};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    $count(x)$: $N(x) - \epsilon m \leq count(x) \leq N(x) + \epsilon m$. \\
                    Heavy Hitters: returns
                    \begin{itemize}
                        \item every item that appears $\geq 2 \epsilon m$ times.
                        \item no item that appears $< \epsilon m$ times.
                    \end{itemize}
                    \begin{lstlisting}[mathescape]
                Set $P$ of $<item, count>$ pairs
                For each $u$ in stream $S$:
                    1. if $<u,c>$ is in set $P$, increment $c$.
                    2. else add $<u,1>$ to set $P$.
                    3. if $|P| > k$, decrement count $c$ for each item.
                    4. Remove all items from $P$ with count $c = 0$.
                
                Count($x$):
                    1. if $<x,c>$ is in $P$, return $c$.
                    2. else return 0.
            \end{lstlisting}
                    Choose $k = 1/\epsilon$. Then $N(x) - \epsilon m \leq count(x) \leq N(x)$. \\
                    Space required: $O(klogm)$. \\
                    Proof:
                    \begin{itemize}
                        \item Count of $x$ is incremented $N(x)$ times in total.
                        \item Total increments is $m$.
                        \item When $count(x)$ is decremented, at least $k$ items are also decremented.
                        \item At most $m$ decrements in total.
                        \item So $count(x)$ is decremented at most $m/k$ times.
                    \end{itemize}
                    For Heavy Hitters: return $x$ if $count(x) \geq \epsilon m$.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 4 Misra Gries};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Cost of each node $i$: $C_i = \sum\limits_i x_{i,j}d(p_i,p_j)$.\\
                    The LP minimizes $min \sum\limits_i C_i$. \\
                    Goal: round fractional LP such that $C_i' \leq 4C_i$.
                    \begin{itemize}
                        \item If some $p_i$ is within $4C_j$ of $p_j$, remove $p_i$ from centers.
                        \item In other words: if there is $q$ s.t. $d(p_i,q) \leq 2C_i$ and $d(p_j,q) \leq 2C_j$,
                              delete $p_i$.
                        \item $C_j \leq C_i$ because of the order of node processing.
                    \end{itemize}
                    Goal: less than $2k$ centers.
                    \begin{itemize}
                        \item $\sum\limits_{i: d(p_i, p_j) \leq 2C_j}y_i \geq 1/2$
                        \item Since y's sum to k, if $V(i)$ are disjoint, cannot add more than $2k$ points to $S$.
                    \end{itemize}
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 6 k-Median Clustering};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Cache size = $M$. Block size = $B$. Number of lines (cache slots) = $M/B$. \\
                    Assumptions:
                    \begin{itemize}
                        \item One cache level.
                        \item Only memory access has cost.
                        \item Ideal cache and replacement.
                    \end{itemize}
                    \begin{tabular}{|l|c|c|}\hline
                        \textbf{Problem}        & \textbf{EMM}                     \\ \hline
                        Scan Array              & $O(N/B)$                         \\
                        Search Array            & $O(log (N/B))$                   \\
                        Sort B-tree             & $O(Nlog_BN)$                     \\
                        Search B-tree           & $O(log_BN)$                      \\
                        Insert/Delete B-tree    & $O(log_BN)$                      \\
                        Search BufferTree       & $O(logN)$                        \\
                        Insert BufferTree       & $O((1/B) logN)$                  \\
                        Ex.MergeSort/BufferTree & $O(\frac{N}{B}log_{M/B}N/B)$     \\
                        BFS                     & $O(n + \frac{m}{B}log_{M/B}m/B)$ \\
                        \hline
                    \end{tabular}

                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 7 External Memory Model};
    \end{tikzpicture}

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    $(a,b)-tree$ with $n$ keys has height $\leq log_a(\frac{n}{a}) + 1$.\\
                    At most $n/a$ leaves. Every node except the root has degree $> a$. \\
                    Node at height $log_a(\frac{n}{a})$ has $\geq a^{log_a(\frac{a}{n})} \geq \frac{n}{a}$ leaves. \\
                    Corollary: if $a\geq B$, then $(a,b)-tree$ with $n$ keys has height $O(log_B n)$. \\
                    Amortized cost of split/share/merge is $O(1/B)$, thus $O((1/B) log_BN)$ per operation. \\
                    With parent points: insert may cost $O(B log_B N)$ if every level needs to split.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Lecture 7 Caching};
    \end{tikzpicture}

    \vfill\null
    \columnbreak

    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.3\textwidth}
                \small{
                    Cache-oblivious structure for searching.
                    \begin{itemize}
                        \item Let $T_r$ be the subtree consisting of all the nodes of depth $\leq \lfloor(logn)/2\rfloor$.
                        \item Let $T_2, T_2, ... , T_k$ be the subtrees consisting of all the nodes of depth $> \lfloor(logn)/2\rfloor$, each subtree rooted at a unique node of depth $\lfloor(logn)/2\rfloor + 1$.
                        \item Then arrange the array as follows: $L(T_r), L(T_1), L(T_2), ... , L(T_k)$.
                    \end{itemize}
                    Single value search: $O(log_Bn)$. \\
                    Range search with $k$ values in range $[k1, k2]$: $O(log_Bn + k/B + 1)$.
                }
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {van Emde Boas Tree};
    \end{tikzpicture}


\end{multicols*}
\end{document}
